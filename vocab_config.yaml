# config.yaml - Configuration file for experiments

# Dataset configurations
# dataset_sizes: [1000, 5000, 25000]  # Profile counts to test
dataset_sizes: [10]
model_sizes: 
#  - "microsoft/DialoGPT-small"    # ~117M parameters (for testing)
#  - "microsoft/DialoGPT-medium"   # ~345M parameters  
#  - "meta-llama/Llama-2-7b-hf"   # 7B parameters
  - "meta-llama/Meta-Llama-3-8B" # 8B parameters

# Data diversification
# num_variations: [1, 5, 10]  # Number of format variations for CPT
num_variations: [1]  # Number of format variations for CPT

# Training hyperparameters
batch_size: 1
max_length: 2048
learning_rate_cpt: 1e-4
learning_rate_ift: 5e-5
num_epochs_cpt: 1
num_epochs_ift: 1

# Output configuration
output_dir: "outputs"

# Vocabulary sizes for comparison
vocab_sizes: [32000, 128000]

# Hardware configuration
fsdp_config: ./fsdp_config.json
use_gpu: true
num_gpus: 1
gradient_accumulation_steps: 4

# Evaluation configuration
eval_batch_size: 16
max_eval_samples: 1000

vocabulary_experiment:
  enable: true
  models: 
    - name: "Llama2-7B"
      path: "meta-llama/Llama-2-7b-hf"
      vocab_size: 32000
    - name: "Llama3-8B" 
      path: "meta-llama/Meta-Llama-3-8B"
      vocab_size: 128000
